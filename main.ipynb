{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data = pd.read_csv(\"../HumanObserved-Dataset/HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\")\n",
    "pos_data = pd.read_csv(\"../HumanObserved-Dataset/HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\")\n",
    "neg_data = pd.read_csv(\"../HumanObserved-Dataset/HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_setting_one(master_data,pos_data):\n",
    "    raw_data_temp = pd.concat([pos_data.set_index('img_id_A'),master_data.set_index('img_id')],axis=1,join='inner').reset_index()\n",
    "    raw_data_feature_concat = pd.concat([raw_data_temp.set_index('img_id_B'),master_data.set_index('img_id')],axis=1,join='inner').reset_index()\n",
    "    raw_data_feature_concat.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "    col_rename = ['img_id_B','img_id_A','target']\n",
    "    for columns in range(1,len(list(raw_data_feature_concat.columns)[3:])+1):\n",
    "        if(columns < 10):\n",
    "            col_rename.append(\"fa\"+str(columns))\n",
    "        else:\n",
    "            col_rename.append(\"fb\"+str(columns - 9))\n",
    "    raw_data_feature_concat.columns = col_rename\n",
    "    col_rename.append(col_rename.pop(2))\n",
    "    temp = col_rename[0]\n",
    "    col_rename[0] = col_rename[1]\n",
    "    col_rename[1] = temp\n",
    "    raw_data_feature_concat = raw_data_feature_concat[col_rename]\n",
    "    return raw_data_feature_concat\n",
    "\n",
    "def create_setting_two(raw_data_feature_concat):\n",
    "    raw_data_feature_subs = raw_data_feature_concat.copy()\n",
    "    for columns in range(1,int((len(list(raw_data_feature_subs.columns))-3)/2+1)):\n",
    "        raw_data_feature_subs['fm'+str(columns)] = abs(raw_data_feature_subs['fa'+str(columns)] - raw_data_feature_subs['fb'+str(columns)])\n",
    "        raw_data_feature_subs.drop('fa'+str(columns),axis=1,inplace=True)\n",
    "        raw_data_feature_subs.drop('fb'+str(columns),axis=1,inplace=True)\n",
    "    col_swap = list(raw_data_feature_subs.columns)\n",
    "    col_swap.append(col_swap.pop(2))\n",
    "    raw_data_feature_subs=raw_data_feature_subs[col_swap]\n",
    "    return raw_data_feature_subs\n",
    "\n",
    "def representativeClustering(data,sizeOfTheCluster,seed):\n",
    "    kmeans = KMeans(n_clusters=sizeOfTheCluster, random_state=seed)\n",
    "    kmeans_data = kmeans.fit_predict(data.iloc[:,data.columns != 'target'])\n",
    "    data = data.join(pd.DataFrame(kmeans_data,columns=[\"kmean_cluster_number\"]))\n",
    "    '''\n",
    "    2D stratified sampling on the target value and the cluster number so that the algorithm which we will \n",
    "    implement will have fair chances of learning all types of data.\n",
    "    '''\n",
    "    train,test_val = train_test_split(data,test_size = 0.2,stratify=data[[\"target\",\"kmean_cluster_number\"]],random_state=seed)\n",
    "    val,test = train_test_split(test_val,test_size = 0.5,stratify=test_val[[\"target\",\"kmean_cluster_number\"]],random_state=seed)\n",
    "    '''\n",
    "    Cluster number is not required now\n",
    "    '''\n",
    "    train = train.drop([\"kmean_cluster_number\"],axis=1)\n",
    "    test = test.drop([\"kmean_cluster_number\"],axis=1)\n",
    "    val = val.drop([\"kmean_cluster_number\"],axis=1)\n",
    "\n",
    "    mu = kmeans.cluster_centers_\n",
    "    return train,test,val,mu\n",
    "\n",
    "# Linear Regression Functions Development\n",
    "def covar(trainData,num_basis):\n",
    "    ''' \n",
    "    Getting the covar over the training data based on number of basics we have implemented\n",
    "    Changed the spread for Gaussian radial basis function\n",
    "    '''\n",
    "    #print(\"Using Uniform Gaussian radial basis function\")\n",
    "    train_transpose = np.transpose(trainData)\n",
    "    iden = np.identity(np.shape(train_transpose)[0])\n",
    "    holdResult = []\n",
    "    for i in range(0,np.shape(train_transpose)[0]):\n",
    "        holdRow = []\n",
    "        for j in range(0,len(trainData)):\n",
    "            holdRow.append(train_transpose.iloc[i,j])\n",
    "        # EDIT HERE FOR PRECISION AND NON UNIFORM RADIAL BASICS\n",
    "       \tiden[i] = np.dot(iden[i],np.dot(np.dot(200,i),np.var(holdRow)))\n",
    "    return iden\n",
    "\n",
    "def genPhi(train,covarMat,num_basis,mu):\n",
    "    '''\n",
    "    Getting the Phi based on the covariance and number of basis\n",
    "    '''\n",
    "    phiMat = np.zeros((len(train),int(num_basis))) \n",
    "    covarMatInv = np.linalg.pinv(covarMat)\n",
    "    for i in range(0,num_basis):\n",
    "        for j in range(0,len(train)):\n",
    "            subsResult = (np.subtract(train.iloc[j,],mu[i,]))\n",
    "            L = np.dot(np.transpose(subsResult),covarMatInv)\n",
    "            R = np.dot(L,subsResult)\n",
    "            phiMat[j][i] = math.exp(-np.dot(0.5,R))\n",
    "    return phiMat\n",
    "\n",
    "def updateWeights(weights,phiMat,train_lab,alpha,lam): \n",
    "    midT = np.dot(np.transpose(weights),phiMat)\n",
    "    deltaL = -(np.subtract(train_lab,midT))\n",
    "    deltaD = np.dot(float(deltaL),phiMat)\n",
    "    deltaE = np.transpose(np.matrix(deltaD)) + np.dot(lam,weights)\n",
    "\n",
    "    delta = np.dot(-alpha,deltaE)\n",
    "    new_weight = weights + delta\n",
    "    return new_weight\n",
    "\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(np.transpose(prev_weight),np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "\n",
    "def GetErms(valData,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(valData)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - valData[i]),2)\n",
    "        if(int(np.around(valData[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(valData)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(valData))))\n",
    "\n",
    "def plotData(log_erms_train,log_erms_val,log_erms_test):\n",
    "    writePlot('log_erms_train',log_erms_train)\n",
    "    writePlot('log_erms_val',log_erms_val)\n",
    "    writePlot('log_erms_test',log_erms_test)\n",
    "    return True\n",
    "\n",
    "def writePlot(filename,log):\n",
    "    df = pd.DataFrame(log)\n",
    "    ax = df.plot(figsize=(10,15))\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "\n",
    "    plt.savefig(('./'+filename+'.png'),bbox_inches='tight')\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_feature_concat_pos = create_setting_one(master_data,pos_data)\n",
    "raw_data_feature_concat_neg = create_setting_one(master_data,neg_data)\n",
    "raw_data_feature_subs_pos = create_setting_two(raw_data_feature_concat_pos)\n",
    "raw_data_feature_subs_neg = create_setting_two(raw_data_feature_concat_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using two times the data to fit all the clusters\n",
    "raw_data_feature_subs_neg = raw_data_feature_concat_neg.sample(n=len(raw_data_feature_concat_pos)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unseen Writer partitions\n",
    "\n",
    "raw_data_feature_concat_pos[['A','A_imgNo']] = raw_data_feature_concat_pos['img_id_A'].str.extract('(\\d\\d\\d\\d)([a-z])', expand=False)\n",
    "raw_data_feature_concat_pos[['B','B_imgNo']] = raw_data_feature_concat_pos['img_id_B'].str.extract('(\\d\\d\\d\\d)([a-z])', expand=False)\n",
    "#raw_data_feature_concat['img_id_A'].str.extract('(?P<writerA>\\d\\d\\d\\d)(?P<imageNo>[abcd])', expand=False)\n",
    "raw_data_feature_concat_neg[['A','A_imgNo']] = raw_data_feature_concat_neg['img_id_A'].str.extract('(\\d\\d\\d\\d)([a-z])', expand=False)\n",
    "raw_data_feature_concat_neg[['B','B_imgNo']] = raw_data_feature_concat_neg['img_id_B'].str.extract('(\\d\\d\\d\\d)([a-z])', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.concat([raw_data_feature_concat_pos,raw_data_feature_concat_neg])\n",
    "#pd.DataFrame.to_csv(raw_data,'../feature_concat_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.iloc[:,2:21]\n",
    "M = 4\n",
    "train,test,val,mu = representativeClustering(data=data,sizeOfTheCluster=M,seed=421)\n",
    "train_lab = train.iloc[:,train.columns == 'target']\n",
    "val_lab = val.iloc[:,val.columns == 'target']\n",
    "test_lab = test.iloc[:,test.columns == 'target']\n",
    "train = train.iloc[:,train.columns != 'target']\n",
    "val = val.iloc[:,val.columns != 'target']\n",
    "test = val.iloc[:,test.columns != 'target']\n",
    "#print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covarMat = covar(train,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\" Getting the covar over the training data based on number of basics we have implemented\")\n",
    "#covarMat = covar(train,M)\n",
    "phiMat = genPhi(train,covarMat,M,mu)\n",
    "valMat = genPhi(val,covarMat,M,mu)\n",
    "testMat = genPhi(test,covarMat,M,mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lab = np.asarray(train_lab)\n",
    "log_erms_val = []\n",
    "log_erms_train = []\n",
    "log_erms_test = []\n",
    "np.random.seed(589)\n",
    "prev_weight = np.matrix(np.random.rand(M,1))\n",
    "alpha = 0.01\n",
    "lam = 0.03\n",
    "\n",
    "for i in range(0,len(train)):\n",
    "    print(\"Iteration: \"+str(i))\n",
    "    prev_weight = updateWeights(prev_weight,phiMat[i],train_lab[i],alpha,lam)\n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "    TR_TEST_OUT   = GetValTest(phiMat,prev_weight) \n",
    "    Erms_TR       = GetErms(np.transpose(TR_TEST_OUT),np.asarray(train_lab))\n",
    "    log_erms_train.append(float(Erms_TR.split(',')[1]))\n",
    "    print ('---------TrainingData Accuracy: ' + Erms_TR + '--------------')\n",
    "\n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "    VAL_TEST_OUT  = GetValTest(valMat,prev_weight) \n",
    "    Erms_Val      = GetErms(np.transpose(VAL_TEST_OUT),np.asarray(val_lab))\n",
    "    log_erms_val.append(float(Erms_Val.split(',')[1]))\n",
    "    print ('---------ValidationData Accuracy: ' + Erms_Val + '--------------')\n",
    "    #---------------TestingData Accuracy---------------------#\n",
    "    TEST_OUT      = GetValTest(testMat,prev_weight) \n",
    "    Erms_Test = GetErms(np.transpose(TEST_OUT),np.asarray(test_lab))\n",
    "    log_erms_test.append(float(Erms_Test.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData(log_erms_train,log_erms_val,log_erms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(val_lab.iloc[:,0],np.array(np.round(VAL_TEST_OUT.reshape(29382,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pd.Series(np.array(train_lab.iloc[:,0]))\n",
    "y_pred = pd.Series(np.array((np.around(TR_TEST_OUT, 0))).ravel())\n",
    "\n",
    "pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
